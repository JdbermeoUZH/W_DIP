import torch
import numpy as np
import torch.fft
from torch.nn import functional as f


def shifter_Kinput(deconv, target, k_input, maxshift=15):
    '''
    This function takes the deconvolved image (deconv) and generated image (target) and shifts the deconv image with
    respect to the target image according to the shift (k_input) of the their respective psf-kernels to each other.
    :deconv: Blurred image that was deconvolved with the auxillary kernel
    :target: Estimate of the sharp image that was generated by the network
    :k_input: Shift between the auxillary and generated kernel. Calculated by the function "shifter_kernel"
    :return: The function returns the shifted deconv image (img_shift) as well as the cropped target (sharp_gen)
    that it was matched to
    '''

    #Crop target image to allow space for shift
    sharp_gen = target[:, :, (maxshift):deconv.shape[2] - (maxshift), (maxshift):deconv.shape[3] - (maxshift)]

    div = (maxshift * 2) + 1
    x = int(k_input / div) - maxshift
    y = k_input % div - maxshift

    img_shift = deconv[:, :, (maxshift + (x)):(deconv.shape[2] - (maxshift + (-x))), (maxshift + (y)):(deconv.shape[3] - (maxshift + (-y)))]

    return img_shift, sharp_gen


def shifter_kernel(mover, target, pad_size, stride=1):
    '''
    This function finds the shift between the auxillary kernel (mover) and that of the generated kernel (target). This
    is done by zero padding the auxillary kernel and then taking crops the size of the target and see which of the crops
    are closest to the target.
    :mover: The auxillary kernel which is padded and cropped
    :target: The generated kernel that is used as target
    :pad_size: The size the auxillary kernel is padded by to then take the kernel crops from to match to the target
    :return: The lowest loss between the shifted auxillary kernel and generated image, the shift parameter, the shifted
    auxillary kernel, the generated kernel.
    '''

    I_shape = mover.shape[2]
    patch_size = [I_shape, I_shape]
    padder = torch.nn.ZeroPad2d(pad_size)
    mover_windows = f.unfold(padder(mover), patch_size, stride=stride)[None, :].permute(3, 0, 1, 2)

    # Re-normalize the windows, since they don't sum up to one anymore
    mover_windows = f.normalize(mover_windows, p=1, dim=3)

    flat_sharp = target[0, 0, :, :].flatten()[None, :]
    sharp_windows = torch.cat(mover_windows.shape[0] * [flat_sharp])[None, None, :].permute(2, 0, 1, 3)

    diff_windows = torch.sum((mover_windows - sharp_windows) ** 2, dim=3).squeeze()
    sum_loss = diff_windows[torch.argmin(diff_windows)]

    mover_window_select = mover_windows[torch.argmin(diff_windows), :, :, :].reshape(1, 1, patch_size[0], patch_size[1])
    sharp_window_select = sharp_windows[0, :, :, :].reshape(1, 1, patch_size[0], patch_size[1])

    return sum_loss, torch.argmin(diff_windows), mover_window_select, sharp_window_select


def guass_gen(k_size=(10, 10), var=3, samp_size=(20, 20)):
    '''
    This function generates a kernel with varaince (var) that is spanned over the space (samp_size). The returned kernel
    is sampled from this space according to (k_size).
    :k_size: This is the sampling density for the generated kernel. By adjusting the size a larger or smaller kernel of
    the same Gaussian can be generated.
    :var: The variance used for the generated Gaussian
    :samp_size: The space over which the generated Gaussian spans and over which can be sampled from.
    :return: The generated Gaussian
    '''
    nx, ny = samp_size
    sx, sy = k_size
    sigma = var

    x = np.linspace(-(nx - 1) / 2, (nx - 1) / 2, sx)
    y = np.linspace(-(ny - 1) / 2, (ny - 1) / 2, sy)

    xv, yv = np.meshgrid(x, y)
    hg = np.exp(- (xv ** 2 + yv ** 2) / (2 * sigma ** 2))
    h = hg / np.sum(hg)

    return h


def convert_psf2otf(ker, size, device):
    '''
    Convert the kernel (ker) to shape of image (size) and apply circular shift to kernel before applying Fourier transform
    :ker: kernel that should be transformed to the Fourier-domain
    :size: size of the image that the kernel should match in the Fourier space
    :device: device on which to perform the function
    :return: return the kernel in the Fourier-domain
    '''
    psf = torch.zeros(size).to(device)
    # circularly shift
    centre = ker.shape[2]//2 + 1
    psf[:, :, :centre, :centre] = ker[:, :, (centre-1):, (centre-1):]
    psf[:, :, :centre, -(centre-1):] = ker[:, :, (centre-1):, :(centre-1)]
    psf[:, :, -(centre-1):, :centre] = ker[:, :, : (centre-1), (centre-1):]
    psf[:, :, -(centre-1):, -(centre-1):] = ker[:, :, :(centre-1), :(centre-1)]
    # compute the otf
    otf = torch.fft.fft2(psf)
    return otf


def wienerF_otf(img, psf, device, C = 0.025):
    '''
    Perform Wiener deconvolution of blurred image (img) with the kernel (psf).
    :img: The input blurred image that should be deconvolved
    :psf: The kernel that should be used for the Wiener-deconvolution
    :device: The device on which to perform the operation
    :C: the constant that should be used for the Wiener-Filter
    :return: The deconvolved image
    '''
    img_ff = torch.fft.fft2(img)
    psf_ff = convert_psf2otf(psf, img.shape, device)

    w_ff = torch.conj(psf_ff) / (torch.abs(psf_ff) ** 2 + C)
    img_sharp_ff = img_ff * w_ff
    img_sharp = torch.fft.ifft2(img_sharp_ff)

    return img_sharp.abs()
